{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af52339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "filters=16\n",
    "size=34\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(filters, filters*2, 3,1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(filters*2, filters, 1,1)\n",
    "        self.conv3 = nn.Conv2d(filters, filters*2, 3,1, padding=0)\n",
    "        self.conv4 = nn.Conv2d(filters*2, filters, 1,1)\n",
    "        self.conv5 = nn.Conv2d(filters, filters*2, 3,1, padding=0)\n",
    "        self.conv6 = nn.Conv2d(filters*2, filters, 1,1)\n",
    "        self.conv7=nn.Conv2d(filters, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        start=torch.zeros(x.shape[0],x.shape[1],size, size)\n",
    "        start[:,:,1:-1,1:-1]=x\n",
    "        start[:,:,-1,:]=start[:,:,1,:]\n",
    "        start[:,:,0,:]=start[:,:,-2,:]\n",
    "        start[:,:,:,0]=start[:,:,:,-2]\n",
    "        start[:,:,:,-1]=start[:,:,:,1]\n",
    "        start[:,:,0,0]=start[:,:,-2,-2]\n",
    "        start[:,:,-1,-1]=start[:,:,1,1]\n",
    "        start[:,:,-1,0]=start[:,:,1,-2]\n",
    "        start[:,:,0,-1]=start[:,:,-2,1]\n",
    "        x=start\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x =F.relu(self.conv2(x))\n",
    "        \n",
    "        start=torch.zeros(x.shape[0],x.shape[1],size, size)\n",
    "        start[:,:,1:-1,1:-1]=x\n",
    "        start[:,:,-1,:]=start[:,:,1,:]\n",
    "        start[:,:,0,:]=start[:,:,-2,:]\n",
    "        start[:,:,:,0]=start[:,:,:,-2]\n",
    "        start[:,:,:,-1]=start[:,:,:,1]\n",
    "        start[:,:,0,0]=start[:,:,-2,-2]\n",
    "        start[:,:,-1,-1]=start[:,:,1,1]\n",
    "        start[:,:,-1,0]=start[:,:,1,-2]\n",
    "        start[:,:,0,-1]=start[:,:,-2,1]\n",
    "        x=start\n",
    "        \n",
    "        \n",
    "        x =F.relu(self.conv3(x))\n",
    "        x =F.relu(self.conv4(x))\n",
    "        start=torch.zeros(x.shape[0],x.shape[1],size, size)\n",
    "        start[:,:,1:-1,1:-1]=x\n",
    "        start[:,:,-1,:]=start[:,:,1,:]\n",
    "        start[:,:,0,:]=start[:,:,-2,:]\n",
    "        start[:,:,:,0]=start[:,:,:,-2]\n",
    "        start[:,:,:,-1]=start[:,:,:,1]\n",
    "        start[:,:,0,0]=start[:,:,-2,-2]\n",
    "        start[:,:,-1,-1]=start[:,:,1,1]\n",
    "        start[:,:,-1,0]=start[:,:,1,-2]\n",
    "        start[:,:,0,-1]=start[:,:,-2,1]\n",
    "        x=start\n",
    "        \n",
    "        \n",
    "        x =F.relu(self.conv5(x))\n",
    "        x =F.relu(self.conv6(x))\n",
    "        x =torch.sigmoid(self.conv7(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "def game(start, steps):\n",
    "    size=start.shape[0]\n",
    "\n",
    "    for step in range(0,steps):\n",
    "        start[-1]=start[1]\n",
    "        start[0]=start[-2]\n",
    "        start[:,0]=start[:,-2]\n",
    "        start[:,-1]=start[:,1]\n",
    "        start[0,0]=start[-2,-2]\n",
    "        start[-1,-1]=start[1,1]\n",
    "        start[-1,0]=start[1,-2]\n",
    "        start[0,-1]=start[-2,1]\n",
    "        new=np.zeros((size,size))\n",
    "        #print(start)\n",
    "        for i in range(1,size-1):\n",
    "            for j in range(1,size-1):\n",
    "                alive=start[i+1,j]+start[i+1,j+1]+start[i,j+1]+start[i+1,j-1]+start[i,j-1]+start[i-1,j]+start[i-1,j+1]++start[i-1,j-1]\n",
    "                if alive==2:\n",
    "                    new[i,j]=start[i,j]\n",
    "                elif alive==3:\n",
    "                    new[i,j]=1\n",
    "                else:\n",
    "                    new[i,j]=0\n",
    "        start=new\n",
    "    #print(start)\n",
    "    return new[1:-1,1:-1]\n",
    "\n",
    "\n",
    "def data(dataSize, size, returnTensor=False):\n",
    "    inputs=[]\n",
    "    outputs=[] \n",
    "    n=3\n",
    "    for data in range(0,dataSize):\n",
    "        starter=np.zeros((size,size))\n",
    "        starter[1:-1, 1:-1]=np.random.randint(0,2,(size-2,size-2))\n",
    "        inputs.append(starter[1:-1, 1:-1])\n",
    "        outputs.append(game(starter, n))\n",
    "    inputs=np.array(inputs)\n",
    "    inputs=inputs.reshape((dataSize,1,size-2,size-2))\n",
    "    outputs=np.array(outputs)\n",
    "    outputs=outputs.reshape((dataSize,1,size-2,size-2))\n",
    "    tensor_x = torch.Tensor(inputs) # transform to torch tensor\n",
    "    tensor_y = torch.Tensor(outputs)\n",
    "\n",
    "    my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "    if returnTensor:\n",
    "        return my_dataset\n",
    "    my_dataloader = DataLoader(my_dataset) # create your dataloader\n",
    "\n",
    "    return my_dataloader\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    missed=0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X=X.repeat(1,filters,1,1)\n",
    "            pred = model(X)\n",
    "            #print(pred)\n",
    "            totalMissed=np.sum(((pred.numpy()>.5)-y.numpy())**2)\n",
    "            missed+=totalMissed\n",
    "            #print(pred)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "    test_loss /= size\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(missed) \n",
    "    return test_loss, missed\n",
    "    \n",
    "    \n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        X=X.repeat(1,filters,1,1)\n",
    "        \n",
    "        pred = model(X)\n",
    "\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "net=Net()\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11e2cdf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.217359 \n",
      "\n",
      "257040.0\n",
      "Avg loss: 0.190209 \n",
      "\n",
      "255908.0\n",
      "Avg loss: 0.188041 \n",
      "\n",
      "256375.0\n",
      "Avg loss: 0.188082 \n",
      "\n",
      "257120.0\n",
      "Avg loss: 0.188000 \n",
      "\n",
      "257034.0\n",
      "Avg loss: 0.187581 \n",
      "\n",
      "256184.0\n",
      "Avg loss: 0.187424 \n",
      "\n",
      "255868.0\n",
      "Avg loss: 0.187808 \n",
      "\n",
      "256661.0\n",
      "Avg loss: 0.187914 \n",
      "\n",
      "256882.0\n",
      "Avg loss: 0.187779 \n",
      "\n",
      "256610.0\n",
      "Avg loss: 0.187611 \n",
      "\n",
      "256269.0\n",
      "Avg loss: 0.188434 \n",
      "\n",
      "257966.0\n",
      "Avg loss: 0.188092 \n",
      "\n",
      "257268.0\n",
      "Avg loss: 0.187715 \n",
      "\n",
      "256501.0\n",
      "Avg loss: 0.188343 \n",
      "\n",
      "257797.0\n",
      "Avg loss: 0.188246 \n",
      "\n",
      "257605.0\n",
      "Avg loss: 0.188110 \n",
      "\n",
      "257333.0\n",
      "Avg loss: 0.187677 \n",
      "\n",
      "256454.0\n",
      "Avg loss: 0.187357 \n",
      "\n",
      "255806.0\n",
      "Avg loss: 0.187253 \n",
      "\n",
      "255608.0\n",
      "Avg loss: 0.187541 \n",
      "\n",
      "256217.0\n",
      "Avg loss: 0.187509 \n",
      "\n",
      "256167.0\n",
      "Avg loss: 0.187442 \n",
      "\n",
      "256050.0\n",
      "Avg loss: 0.187788 \n",
      "\n",
      "256778.0\n",
      "Avg loss: 0.187276 \n",
      "\n",
      "255759.0\n",
      "Avg loss: 0.187318 \n",
      "\n",
      "255878.0\n",
      "Avg loss: 0.187881 \n",
      "\n",
      "257060.0\n",
      "Avg loss: 0.187641 \n",
      "\n",
      "256612.0\n",
      "Avg loss: 0.187223 \n",
      "\n",
      "255797.0\n",
      "Avg loss: 0.186790 \n",
      "\n",
      "254962.0\n",
      "Avg loss: 0.186599 \n",
      "\n",
      "254631.0\n",
      "Avg loss: 0.187224 \n",
      "\n",
      "255971.0\n",
      "Avg loss: 0.187599 \n",
      "\n",
      "256823.0\n",
      "Avg loss: 0.187733 \n",
      "\n",
      "257181.0\n",
      "Avg loss: 0.187370 \n",
      "\n",
      "256568.0\n",
      "Avg loss: 0.187099 \n",
      "\n",
      "256159.0\n",
      "Avg loss: 0.187058 \n",
      "\n",
      "256253.0\n",
      "Avg loss: 0.187052 \n",
      "\n",
      "256471.0\n",
      "Avg loss: 0.186874 \n",
      "\n",
      "256382.0\n",
      "Avg loss: 0.186557 \n",
      "\n",
      "256145.0\n",
      "Avg loss: 0.186733 \n",
      "\n",
      "256960.0\n",
      "Avg loss: 0.186354 \n",
      "\n",
      "256804.0\n",
      "Avg loss: 0.185333 \n",
      "\n",
      "255564.0\n",
      "Avg loss: 0.185479 \n",
      "\n",
      "256913.0\n",
      "Avg loss: 0.184788 \n",
      "\n",
      "256700.0\n",
      "Avg loss: 0.183860 \n",
      "\n",
      "256358.0\n",
      "Avg loss: 0.183708 \n",
      "\n",
      "257618.0\n",
      "Avg loss: 0.182594 \n",
      "\n",
      "257462.0\n",
      "Avg loss: 0.180856 \n",
      "\n",
      "256280.0\n",
      "Avg loss: 0.180827 \n",
      "\n",
      "258316.0\n",
      "Avg loss: 0.179057 \n",
      "\n",
      "256821.0\n",
      "Avg loss: 0.178184 \n",
      "\n",
      "258230.0\n",
      "Avg loss: 0.176228 \n",
      "\n",
      "256411.0\n",
      "Avg loss: 0.176024 \n",
      "\n",
      "257760.0\n",
      "Avg loss: 0.175310 \n",
      "\n",
      "257315.0\n",
      "Avg loss: 0.174515 \n",
      "\n",
      "256308.0\n",
      "Avg loss: 0.174270 \n",
      "\n",
      "256705.0\n",
      "Avg loss: 0.173173 \n",
      "\n",
      "255547.0\n",
      "Avg loss: 0.173213 \n",
      "\n",
      "255984.0\n",
      "Avg loss: 0.172786 \n",
      "\n",
      "255668.0\n",
      "Avg loss: 0.173014 \n",
      "\n",
      "256657.0\n",
      "Avg loss: 0.172055 \n",
      "\n",
      "255228.0\n",
      "Avg loss: 0.172042 \n",
      "\n",
      "255544.0\n",
      "Avg loss: 0.172377 \n",
      "\n",
      "256850.0\n",
      "Avg loss: 0.172154 \n",
      "\n",
      "256549.0\n",
      "Avg loss: 0.172057 \n",
      "\n",
      "256077.0\n",
      "Avg loss: 0.171481 \n",
      "\n",
      "255381.0\n",
      "Avg loss: 0.171972 \n",
      "\n",
      "256908.0\n",
      "Avg loss: 0.171081 \n",
      "\n",
      "255396.0\n",
      "Avg loss: 0.172243 \n",
      "\n",
      "257115.0\n",
      "Avg loss: 0.171811 \n",
      "\n",
      "257003.0\n",
      "Avg loss: 0.171161 \n",
      "\n",
      "256492.0\n",
      "Avg loss: 0.170589 \n",
      "\n",
      "254667.0\n",
      "Avg loss: 0.171343 \n",
      "\n",
      "256973.0\n",
      "Avg loss: 0.171545 \n",
      "\n",
      "257725.0\n",
      "Avg loss: 0.170720 \n",
      "\n",
      "255742.0\n",
      "Avg loss: 0.170949 \n",
      "\n",
      "257404.0\n",
      "Avg loss: 0.169565 \n",
      "\n",
      "254677.0\n",
      "Avg loss: 0.170178 \n",
      "\n",
      "256417.0\n",
      "Avg loss: 0.169923 \n",
      "\n",
      "256274.0\n",
      "Avg loss: 0.170358 \n",
      "\n",
      "257017.0\n",
      "Avg loss: 0.169655 \n",
      "\n",
      "255714.0\n",
      "Avg loss: 0.169691 \n",
      "\n",
      "256073.0\n",
      "Avg loss: 0.169798 \n",
      "\n",
      "256280.0\n",
      "Avg loss: 0.170123 \n",
      "\n",
      "257008.0\n",
      "Avg loss: 0.169175 \n",
      "\n",
      "255716.0\n",
      "Avg loss: 0.168935 \n",
      "\n",
      "255635.0\n",
      "Avg loss: 0.169305 \n",
      "\n",
      "256203.0\n",
      "Avg loss: 0.168614 \n",
      "\n",
      "255946.0\n",
      "Avg loss: 0.169434 \n",
      "\n",
      "256751.0\n",
      "Avg loss: 0.169526 \n",
      "\n",
      "257294.0\n",
      "Avg loss: 0.169025 \n",
      "\n",
      "256698.0\n",
      "Avg loss: 0.168726 \n",
      "\n",
      "256807.0\n",
      "Avg loss: 0.169308 \n",
      "\n",
      "257731.0\n",
      "Avg loss: 0.168816 \n",
      "\n",
      "256677.0\n",
      "Avg loss: 0.168367 \n",
      "\n",
      "256664.0\n",
      "Avg loss: 0.167979 \n",
      "\n",
      "255124.0\n",
      "Avg loss: 0.168056 \n",
      "\n",
      "256089.0\n",
      "Avg loss: 0.168022 \n",
      "\n",
      "255693.0\n",
      "Avg loss: 0.168026 \n",
      "\n",
      "256637.0\n",
      "First Era!\n",
      "Avg loss: 0.168571 \n",
      "\n",
      "257471.0\n",
      "Avg loss: 0.167999 \n",
      "\n",
      "256262.0\n",
      "Avg loss: 0.168077 \n",
      "\n",
      "256923.0\n",
      "Avg loss: 0.168591 \n",
      "\n",
      "257842.0\n",
      "Avg loss: 0.167741 \n",
      "\n",
      "256292.0\n",
      "Avg loss: 0.167734 \n",
      "\n",
      "255962.0\n",
      "Avg loss: 0.168415 \n",
      "\n",
      "257352.0\n",
      "Avg loss: 0.167764 \n",
      "\n",
      "256462.0\n",
      "Avg loss: 0.167587 \n",
      "\n",
      "255644.0\n",
      "Avg loss: 0.167964 \n",
      "\n",
      "256029.0\n",
      "Avg loss: 0.167948 \n",
      "\n",
      "256506.0\n",
      "Avg loss: 0.167839 \n",
      "\n",
      "256438.0\n",
      "Avg loss: 0.167468 \n",
      "\n",
      "255470.0\n",
      "Avg loss: 0.167326 \n",
      "\n",
      "255665.0\n",
      "Avg loss: 0.167561 \n",
      "\n",
      "256153.0\n",
      "Avg loss: 0.168466 \n",
      "\n",
      "257510.0\n",
      "Avg loss: 0.168665 \n",
      "\n",
      "258031.0\n",
      "Avg loss: 0.167357 \n",
      "\n",
      "255541.0\n",
      "Avg loss: 0.167234 \n",
      "\n",
      "254926.0\n",
      "Avg loss: 0.167751 \n",
      "\n",
      "256756.0\n",
      "Avg loss: 0.168277 \n",
      "\n",
      "257540.0\n",
      "Avg loss: 0.168451 \n",
      "\n",
      "257418.0\n",
      "Avg loss: 0.166879 \n",
      "\n",
      "255351.0\n",
      "Avg loss: 0.168092 \n",
      "\n",
      "256753.0\n",
      "Avg loss: 0.167944 \n",
      "\n",
      "256745.0\n",
      "Avg loss: 0.167250 \n",
      "\n",
      "255536.0\n",
      "Avg loss: 0.168643 \n",
      "\n",
      "257869.0\n",
      "Avg loss: 0.167744 \n",
      "\n",
      "256702.0\n",
      "Avg loss: 0.168954 \n",
      "\n",
      "259106.0\n",
      "Avg loss: 0.167896 \n",
      "\n",
      "256617.0\n",
      "Avg loss: 0.167915 \n",
      "\n",
      "257171.0\n",
      "Avg loss: 0.166794 \n",
      "\n",
      "254823.0\n",
      "Avg loss: 0.167877 \n",
      "\n",
      "256392.0\n",
      "Avg loss: 0.167839 \n",
      "\n",
      "257015.0\n",
      "Avg loss: 0.167807 \n",
      "\n",
      "257017.0\n",
      "Avg loss: 0.167231 \n",
      "\n",
      "256042.0\n",
      "Avg loss: 0.167395 \n",
      "\n",
      "255618.0\n",
      "Avg loss: 0.168188 \n",
      "\n",
      "257593.0\n",
      "Avg loss: 0.167227 \n",
      "\n",
      "255912.0\n",
      "Avg loss: 0.167434 \n",
      "\n",
      "255786.0\n",
      "Avg loss: 0.166916 \n",
      "\n",
      "255408.0\n",
      "Avg loss: 0.168093 \n",
      "\n",
      "257209.0\n",
      "Avg loss: 0.167174 \n",
      "\n",
      "256001.0\n",
      "Avg loss: 0.167681 \n",
      "\n",
      "256661.0\n",
      "Avg loss: 0.167258 \n",
      "\n",
      "255908.0\n",
      "Avg loss: 0.167075 \n",
      "\n",
      "255463.0\n",
      "Avg loss: 0.168309 \n",
      "\n",
      "258183.0\n",
      "Avg loss: 0.167629 \n",
      "\n",
      "256648.0\n",
      "Avg loss: 0.166922 \n",
      "\n",
      "255684.0\n",
      "Avg loss: 0.167634 \n",
      "\n",
      "256619.0\n",
      "Avg loss: 0.167180 \n",
      "\n",
      "256101.0\n",
      "Avg loss: 0.167311 \n",
      "\n",
      "256466.0\n",
      "Avg loss: 0.167370 \n",
      "\n",
      "256534.0\n",
      "Avg loss: 0.167684 \n",
      "\n",
      "257224.0\n",
      "Avg loss: 0.167524 \n",
      "\n",
      "256983.0\n",
      "Avg loss: 0.166685 \n",
      "\n",
      "255167.0\n",
      "Avg loss: 0.166834 \n",
      "\n",
      "255338.0\n",
      "Avg loss: 0.167310 \n",
      "\n",
      "256244.0\n",
      "Avg loss: 0.166659 \n",
      "\n",
      "255006.0\n",
      "Avg loss: 0.167243 \n",
      "\n",
      "256256.0\n",
      "Avg loss: 0.167327 \n",
      "\n",
      "256353.0\n",
      "Avg loss: 0.167379 \n",
      "\n",
      "256889.0\n",
      "Avg loss: 0.167621 \n",
      "\n",
      "257168.0\n",
      "Avg loss: 0.167461 \n",
      "\n",
      "256733.0\n",
      "Avg loss: 0.167863 \n",
      "\n",
      "257647.0\n",
      "Avg loss: 0.166674 \n",
      "\n",
      "255374.0\n",
      "Avg loss: 0.167602 \n",
      "\n",
      "257133.0\n",
      "Avg loss: 0.167431 \n",
      "\n",
      "256858.0\n",
      "Avg loss: 0.167960 \n",
      "\n",
      "257434.0\n",
      "Avg loss: 0.167378 \n",
      "\n",
      "257226.0\n",
      "Avg loss: 0.167416 \n",
      "\n",
      "257100.0\n",
      "Avg loss: 0.166928 \n",
      "\n",
      "256204.0\n",
      "Avg loss: 0.166350 \n",
      "\n",
      "255086.0\n",
      "Avg loss: 0.167503 \n",
      "\n",
      "257329.0\n",
      "Avg loss: 0.165850 \n",
      "\n",
      "254056.0\n",
      "Avg loss: 0.166967 \n",
      "\n",
      "256139.0\n",
      "Avg loss: 0.166852 \n",
      "\n",
      "256027.0\n",
      "Avg loss: 0.167521 \n",
      "\n",
      "256806.0\n",
      "Avg loss: 0.167677 \n",
      "\n",
      "257148.0\n",
      "Avg loss: 0.167324 \n",
      "\n",
      "256337.0\n",
      "Avg loss: 0.167327 \n",
      "\n",
      "256347.0\n",
      "Avg loss: 0.167156 \n",
      "\n",
      "256619.0\n",
      "Avg loss: 0.166432 \n",
      "\n",
      "255859.0\n",
      "Avg loss: 0.166342 \n",
      "\n",
      "255270.0\n",
      "Avg loss: 0.166667 \n",
      "\n",
      "255995.0\n",
      "Avg loss: 0.166556 \n",
      "\n",
      "255023.0\n",
      "Avg loss: 0.167141 \n",
      "\n",
      "256611.0\n",
      "Avg loss: 0.167063 \n",
      "\n",
      "256310.0\n",
      "Avg loss: 0.167130 \n",
      "\n",
      "256411.0\n",
      "Avg loss: 0.167048 \n",
      "\n",
      "256188.0\n",
      "Avg loss: 0.166907 \n",
      "\n",
      "255955.0\n",
      "Avg loss: 0.166719 \n",
      "\n",
      "256240.0\n",
      "Avg loss: 0.166810 \n",
      "\n",
      "256316.0\n",
      "Avg loss: 0.167433 \n",
      "\n",
      "256909.0\n",
      "Avg loss: 0.167322 \n",
      "\n",
      "257347.0\n",
      "Avg loss: 0.167142 \n",
      "\n",
      "257252.0\n",
      "Avg loss: 0.167327 \n",
      "\n",
      "257353.0\n",
      "Avg loss: 0.166934 \n",
      "\n",
      "256407.0\n",
      "Avg loss: 0.166662 \n",
      "\n",
      "255816.0\n",
      "Avg loss: 0.166708 \n",
      "\n",
      "255924.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss=[]\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    #print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    my_dataloader=data(1000,34)\n",
    "    train_loop(my_dataloader, net, loss_fn, optimizer)\n",
    "    if t %10==0:\n",
    "        loss.append(test_loop(my_dataloader, net, loss_fn)) \n",
    "print(\"First Era!\") \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate*.1)\n",
    "for t in range(epochs):\n",
    "    #print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    my_dataloader=data(1000,34)\n",
    "    train_loop(my_dataloader, net, loss_fn, optimizer)\n",
    "    if t %10==0:\n",
    "        loss.append(test_loop(my_dataloader, net, loss_fn)) \n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bccb97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=np.array(loss)\n",
    "np.savetxt(\"unconstrainedperiodicove16n3.csv\", loss, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3754ebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gol_venv",
   "language": "python",
   "name": "gol_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
