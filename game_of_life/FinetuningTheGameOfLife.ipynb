{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb76de8-64cd-441e-aefd-a28a004a0443",
   "metadata": {},
   "source": [
    "# Fine Tuning The Game of Life\n",
    "\n",
    "The experiement to be ran here is the following. We will take `ethan_m8_n2_checkpoint1000.pt` as our base model since the `ethan` model converged to 0/1000 examples wrong within 200 training epochs after the 1000th epoch checkpoint. However, the model still gets 1000/1000 examples wrong at this checkpoint, indicating that it is close to a viable solution but not quite at one. We will copy this model three times and perturb the first layer weights slightly. Then, we will fine tune for another 1000 epochs and see the resulting models. The current hypothesis is that the behavior of the intermediary step is entirely dependent on the example data seen. So two of the perturbed models will fine tune on the same data. The last model will fine tune on different data. That way, we can hopefully answer the following questions:\n",
    "\n",
    "1) Is the intermediary behavior dependent on the data scene?\n",
    "2) Will enough variability in the intermediary layer be introduced by fine tuning a model very close to convergence?\n",
    "3) Is a model that is close to convergence still close to converging after slight perturbations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07d5538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from GOLDataset import generateDataset\n",
    "from GOLCNN import OPNet, train_epoch, test_model\n",
    "from MinimalSolution import MinNet\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a9d54d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fee9fda0db0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed everything for reproducibility\n",
    "# seed = 14 for irene, jeff, karol m=8, n=2\n",
    "seed = 14\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7202d-6e25-4d74-8985-76458a199c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "name1 = 'irene'\n",
    "name2 = 'jeff'\n",
    "name3 = 'karol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32629be1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "dataset_size = 1000\n",
    "datapoint_size = 32\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "perturbation_coefficient = 1e-2\n",
    "batch_size_param = 1\n",
    "checkpoint_epochs = 1000\n",
    "epochs = 500\n",
    "checkpoint_rate = 100\n",
    "\n",
    "m = 8 # Overparameterization Factor\n",
    "n = 2  # Steps of GOL simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06556d1d-e362-4b70-b7c7-6f111e7861cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model checkpoints loaded successfully\n"
     ]
    }
   ],
   "source": [
    "model1 = torch.load(f'./models/ethan_m8_n2_checkpoint1000.pt')\n",
    "model2 = torch.load(f'./models/ethan_m8_n2_checkpoint1000.pt')\n",
    "model3 = torch.load(f'./models/ethan_m8_n2_checkpoint1000.pt')\n",
    "\n",
    "criterion1 = nn.MSELoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "criterion3 = nn.MSELoss()\n",
    "\n",
    "optimizer1 = torch.optim.SGD(model1.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.SGD(model2.parameters(), lr=learning_rate)\n",
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=learning_rate)\n",
    "\n",
    "print('model checkpoints loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bd5115-6d6a-4ce2-82da-03f766760909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irene: Accuracy: 0.0, Test Loss: 0.05050164833664894, Correct: 0/1000, Incorrect: 1000/1000\n"
     ]
    }
   ],
   "source": [
    "# Check the integrity of loaded model\n",
    "# Expected loss around 0.05\n",
    "# Expected number Incorrect around 1000/1000\n",
    "dataloader = generateDataset(dataSetSize=dataset_size, size=32, n_steps=2)\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(model1, dataloader, m, criterion1)\n",
    "print(f'{name1}: Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87dc2bd7-572c-4004-8481-17c416bb9625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perturb first layer model weights\n",
    "fl_weight_shape = model1.type1_layers[0].weight.shape\n",
    "random_tensor = torch.rand(fl_weight_shape).to(device)\n",
    "perturbation_tensor = (random_tensor - 0.5)*(perturbation_coefficient)\n",
    "model1.type1_layers[0].weight.data += perturbation_tensor\n",
    "\n",
    "random_tensor2 = torch.rand(fl_weight_shape).to(device)\n",
    "perturbation_tensor2 = (random_tensor2 - 0.5)*(perturbation_coefficient)\n",
    "model2.type1_layers[0].weight.data += perturbation_tensor2\n",
    "\n",
    "random_tensor3 = torch.rand(fl_weight_shape).to(device)\n",
    "perturbation_tensor3 = (random_tensor3 - 0.5)*(perturbation_coefficient)\n",
    "model3.type1_layers[0].weight.data += perturbation_tensor3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a096f743-4018-4358-a6d6-992cff1152e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irene: Accuracy: 0.0, Test Loss: 0.05237091705203056, Correct: 0/1000, Incorrect: 1000/1000\n",
      "Jeff: Accuracy: 0.0, Test Loss: 0.05266406759619713, Correct: 0/1000, Incorrect: 1000/1000\n",
      "Karol: Accuracy: 0.0, Test Loss: 0.05132685601711273, Correct: 0/1000, Incorrect: 1000/1000\n"
     ]
    }
   ],
   "source": [
    "# Check the integrity of perturbed models\n",
    "# Expected loss around 0.0523\n",
    "# Expected number Incorrect around 1000/1000\n",
    "dataloader = generateDataset(dataSetSize=dataset_size, size=32, n_steps=2)\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(model1, dataloader, m, criterion1)\n",
    "print(f'{name1}: Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')\n",
    "\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(model2, dataloader, m, criterion2)\n",
    "print(f'{name2}: Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')\n",
    "\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(model3, dataloader, m, criterion3)\n",
    "print(f'{name3}: Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d983d888-2403-447e-bc7d-88a12bf28ced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000\n",
      "Irene: Epoch: 1000/1500, Test Loss: 0.05025511234998703, Incorrect: 1000/1000 examples\n",
      "Jeff: Epoch: 1000/1500, Test Loss: 0.05021347105503082, Incorrect: 1000/1000 examples\n",
      "Karol: Epoch: 1000/1500, Test Loss: 0.05021686479449272, Incorrect: 1000/1000 examples\n",
      "Epoch: 1100\n",
      "Irene: Epoch: 1100/1500, Test Loss: 0.0018787500448524952, Incorrect: 51/1000 examples\n",
      "Jeff: Epoch: 1100/1500, Test Loss: 0.0018816253868862987, Incorrect: 51/1000 examples\n",
      "Karol: Epoch: 1100/1500, Test Loss: 0.0018658285262063146, Incorrect: 57/1000 examples\n",
      "Epoch: 1200\n",
      "Irene: Epoch: 1200/1500, Test Loss: 0.0005328803090378642, Incorrect: 0/1000 examples\n",
      "Jeff: Epoch: 1200/1500, Test Loss: 0.0005357351037673652, Incorrect: 0/1000 examples\n",
      "Karol: Epoch: 1200/1500, Test Loss: 0.0005295646260492504, Incorrect: 0/1000 examples\n",
      "Epoch: 1300\n",
      "Irene: Epoch: 1300/1500, Test Loss: 0.00026804316439665854, Incorrect: 0/1000 examples\n",
      "Jeff: Epoch: 1300/1500, Test Loss: 0.0002687508531380445, Incorrect: 0/1000 examples\n",
      "Karol: Epoch: 1300/1500, Test Loss: 0.0002667194348759949, Incorrect: 0/1000 examples\n",
      "Epoch: 1400\n",
      "Irene: Epoch: 1400/1500, Test Loss: 0.00016762364248279482, Incorrect: 0/1000 examples\n",
      "Jeff: Epoch: 1400/1500, Test Loss: 0.00016878869791980833, Incorrect: 0/1000 examples\n",
      "Karol: Epoch: 1400/1500, Test Loss: 0.00016783819592092186, Incorrect: 0/1000 examples\n",
      "Epoch: 1500\n",
      "Irene: Epoch: 1500/1500, Test Loss: 0.00011740207264665514, Incorrect: 0/1000 examples\n",
      "Jeff: Epoch: 1500/1500, Test Loss: 0.00011835755867650732, Incorrect: 0/1000 examples\n",
      "Karol: Epoch: 1500/1500, Test Loss: 0.0001175595898530446, Incorrect: 0/1000 examples\n"
     ]
    }
   ],
   "source": [
    "# Training Sequence\n",
    "full_data1 = []\n",
    "full_data2 = []\n",
    "full_data3 = []\n",
    "\n",
    "checkpoint_data1 = []\n",
    "checkpoint_data2 = []\n",
    "checkpoint_data3 = []\n",
    "\n",
    "for t in range(checkpoint_epochs, checkpoint_epochs + epochs + 1):\n",
    "    dataloader = generateDataset(dataSetSize=dataset_size, \n",
    "                                 size=datapoint_size, \n",
    "                                 n_steps=n)\n",
    "    \n",
    "    epoch_train_loss1 = train_epoch(model1, optimizer1, criterion1, dataloader, m)\n",
    "    full_data1.append([t, epoch_train_loss1])\n",
    "    \n",
    "    epoch_train_loss2 = train_epoch(model2, optimizer2, criterion2, dataloader, m)\n",
    "    full_data2.append([t, epoch_train_loss2])\n",
    "    \n",
    "    dataloader2 = generateDataset(dataSetSize=dataset_size, \n",
    "                                 size=datapoint_size, \n",
    "                                 n_steps=n)\n",
    "    \n",
    "    epoch_train_loss3 = train_epoch(model3, optimizer3, criterion3, dataloader2, m)\n",
    "    full_data3.append([t, epoch_train_loss3])\n",
    "    \n",
    "    if t % checkpoint_rate == 0:\n",
    "        print(f'Epoch: {t}')\n",
    "              \n",
    "        acc1, epoch_test_loss1, num_correct1, num_wrong1 = test_model(model1, dataloader, m, criterion1)\n",
    "        checkpoint_name1 = f'{name1}_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data1.append([t, checkpoint_name1, acc1, epoch_test_loss1, num_correct1, num_wrong1])\n",
    "        print(f'{name1}: Epoch: {t}/{checkpoint_epochs + epochs}, Test Loss: {epoch_test_loss1}, Incorrect: {num_wrong1}/1000 examples')\n",
    "        torch.save(model1, f'./models/{checkpoint_name1}')\n",
    "        \n",
    "        acc2, epoch_test_loss2, num_correct2, num_wrong2 = test_model(model2, dataloader, m, criterion2)\n",
    "        checkpoint_name2 = f'{name2}_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data2.append([t, checkpoint_name2, acc2, epoch_test_loss2, num_correct2, num_wrong2])\n",
    "        print(f'{name2}: Epoch: {t}/{checkpoint_epochs + epochs}, Test Loss: {epoch_test_loss2}, Incorrect: {num_wrong2}/1000 examples')\n",
    "        torch.save(model2, f'./models/{checkpoint_name2}')\n",
    "        \n",
    "        acc3, epoch_test_loss3, num_correct3, num_wrong3 = test_model(model3, dataloader, m, criterion3)\n",
    "        checkpoint_name3 = f'{name3}_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data3.append([t, checkpoint_name3, acc3, epoch_test_loss3, num_correct3, num_wrong3])\n",
    "        print(f'{name3}: Epoch: {t}/{checkpoint_epochs + epochs}, Test Loss: {epoch_test_loss3}, Incorrect: {num_wrong3}/1000 examples')\n",
    "        torch.save(model3, f'./models/{checkpoint_name3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46124356-c111-4b50-98c7-23134b0b738c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_full_data1 = pd.DataFrame(full_data1, columns =['epoch', 'training_loss'])\n",
    "df_full_data2 = pd.DataFrame(full_data2, columns =['epoch', 'training_loss'])\n",
    "df_full_data3 = pd.DataFrame(full_data3, columns =['epoch', 'training_loss'])\n",
    "\n",
    "df_checkpoint_data1 = pd.DataFrame(checkpoint_data1, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])\n",
    "df_checkpoint_data2 = pd.DataFrame(checkpoint_data2, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])\n",
    "df_checkpoint_data3 = pd.DataFrame(checkpoint_data3, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d816d574-1f01-468d-9347-9eb109ce9ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_full_data1.to_csv(f'./logs/{name1}_full_data.csv')\n",
    "df_full_data2.to_csv(f'./logs/{name2}_full_data.csv')\n",
    "df_full_data3.to_csv(f'./logs/{name3}_full_data.csv')\n",
    "\n",
    "df_checkpoint_data1.to_csv(f'./logs/{name1}_checkpoint_data.csv')\n",
    "df_checkpoint_data2.to_csv(f'./logs/{name2}_checkpoint_data.csv')\n",
    "df_checkpoint_data3.to_csv(f'./logs/{name3}_checkpoint_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gol_venv",
   "language": "python",
   "name": "gol_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
