{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a064907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from GOLDataset import generateDataset\n",
    "from GOLCNN import OPNet, train_epoch, test_model\n",
    "from MinimalSolution import MinNet\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70df9be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd191b8a610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed everything for reproducibility\n",
    "# seed = 11 for carl and denise m=16, n=2\n",
    "# seed = 12 for ethan and fred m=8, n=2\n",
    "# seed = 13 for greg and harold m=8, n=2\n",
    "seed = 13\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbfc8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0, Test Loss: 3.1796076216035555e-18, Correct: 1000/1000, Incorrect: 0/1000\n"
     ]
    }
   ],
   "source": [
    "# Ensure test_model() works on the minimal solution CNN\n",
    "dataset_size = 1000\n",
    "dataloader = generateDataset(dataSetSize=dataset_size, size=32, n_steps=3)\n",
    "min_model = MinNet(3)\n",
    "min_model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(min_model, dataloader, 1, criterion)\n",
    "print(f'Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3241bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "dataset_size = 1000\n",
    "datapoint_size = 32\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size_param = 1\n",
    "epochs = 1500\n",
    "era2epochs = 0\n",
    "checkpoint_rate = 100\n",
    "\n",
    "m = 8 # Overparameterization Factor\n",
    "n = 2  # Steps of GOL simulation\n",
    "\n",
    "model_amber = OPNet(m, n)\n",
    "model_brian = OPNet(m, n)\n",
    "\n",
    "criterion_amber = nn.MSELoss()\n",
    "criterion_brian = nn.MSELoss()\n",
    "optimizer_amber = torch.optim.SGD(model_amber.parameters(), lr=learning_rate)\n",
    "optimizer_brian = torch.optim.SGD(model_brian.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models loaded to device\n"
     ]
    }
   ],
   "source": [
    "model_amber.to(device)\n",
    "model_brian.to(device)\n",
    "print('models loaded to device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d195285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greg: Epoch: 100/1500, Test Loss: 0.1765715330839157, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 100/1500, Test Loss: 0.17821846902370453, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 200/1500, Test Loss: 0.17261932790279388, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 200/1500, Test Loss: 0.16698187589645386, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 300/1500, Test Loss: 0.16792279481887817, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 300/1500, Test Loss: 0.15850381553173065, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 400/1500, Test Loss: 0.15910843014717102, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 400/1500, Test Loss: 0.1475035697221756, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 500/1500, Test Loss: 0.1488684117794037, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 500/1500, Test Loss: 0.13822147250175476, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 600/1500, Test Loss: 0.14163683354854584, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 600/1500, Test Loss: 0.12569941580295563, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 700/1500, Test Loss: 0.13257110118865967, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 700/1500, Test Loss: 0.08694077283143997, Incorrect: 1000/1000 examples\n",
      "Greg: Epoch: 800/1500, Test Loss: 0.11536581069231033, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 800/1500, Test Loss: 0.016453208401799202, Incorrect: 28/1000 examples\n",
      "Greg: Epoch: 900/1500, Test Loss: 0.0674806758761406, Incorrect: 1000/1000 examples\n",
      "Harold: Epoch: 900/1500, Test Loss: 0.0041566286236047745, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1000/1500, Test Loss: 0.010562624782323837, Incorrect: 89/1000 examples\n",
      "Harold: Epoch: 1000/1500, Test Loss: 0.002178504830226302, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1100/1500, Test Loss: 0.0023644184693694115, Incorrect: 0/1000 examples\n",
      "Harold: Epoch: 1100/1500, Test Loss: 0.0014420575462281704, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1200/1500, Test Loss: 0.0010836506262421608, Incorrect: 0/1000 examples\n",
      "Harold: Epoch: 1200/1500, Test Loss: 0.0010645949514582753, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1300/1500, Test Loss: 0.0006550904363393784, Incorrect: 0/1000 examples\n",
      "Harold: Epoch: 1300/1500, Test Loss: 0.0008380215149372816, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1400/1500, Test Loss: 0.0004588479350786656, Incorrect: 0/1000 examples\n",
      "Harold: Epoch: 1400/1500, Test Loss: 0.0006929931696504354, Incorrect: 0/1000 examples\n",
      "Greg: Epoch: 1500/1500, Test Loss: 0.0003429784846957773, Incorrect: 0/1000 examples\n",
      "Harold: Epoch: 1500/1500, Test Loss: 0.0005837175413034856, Incorrect: 0/1000 examples\n",
      "END OF ERA 1\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "full_data_amber = []\n",
    "full_data_brian = []\n",
    "checkpoint_data_amber = []\n",
    "checkpoint_data_brian = []\n",
    "\n",
    "for t in range(1, epochs + 1):\n",
    "    dataloader = generateDataset(dataSetSize=dataset_size, \n",
    "                                 size=datapoint_size, \n",
    "                                 n_steps=n)\n",
    "    \n",
    "    epoch_train_loss_amber = train_epoch(model_amber, optimizer_amber, criterion_amber, dataloader, m)\n",
    "    full_data_amber.append([t, epoch_train_loss_amber])\n",
    "    \n",
    "    epoch_train_loss_brian = train_epoch(model_brian, optimizer_brian, criterion_brian, dataloader, m)\n",
    "    full_data_brian.append([t, epoch_train_loss_brian])\n",
    "    \n",
    "    if t % checkpoint_rate == 0:\n",
    "        acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber = test_model(model_amber, dataloader, m, criterion_amber)\n",
    "        checkpoint_name_amber = f'greg_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_amber.append([t, checkpoint_name_amber, acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber])\n",
    "        print(f'Greg: Epoch: {t}/{epochs}, Test Loss: {epoch_test_loss_amber}, Incorrect: {num_wrong_amber}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_amber}')\n",
    "        \n",
    "        acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian = test_model(model_brian, dataloader, m, criterion_brian)\n",
    "        checkpoint_name_brian = f'harold_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_brian.append([t, checkpoint_name_brian, acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian])\n",
    "        print(f'Harold: Epoch: {t}/{epochs}, Test Loss: {epoch_test_loss_brian}, Incorrect: {num_wrong_brian}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_brian}')\n",
    "        \n",
    "print(\"END OF ERA 1\")\n",
    "\n",
    "# optimizer_amber = torch.optim.SGD(model_amber.parameters(), lr=learning_rate*0.1)\n",
    "# optimizer_brian = torch.optim.SGD(model_brian.parameters(), lr=learning_rate*0.1)\n",
    "\n",
    "# for t in range(epochs + 1, epochs+era2epochs+1):\n",
    "#     dataloader = generateDataset(dataSetSize=dataset_size, \n",
    "#                                  size=datapoint_size, \n",
    "#                                  n_steps=n)\n",
    "    \n",
    "#     epoch_train_loss_amber = train_epoch(model_amber, optimizer_amber, criterion_amber, dataloader, m)\n",
    "#     full_data_amber.append([t, epoch_train_loss_amber])\n",
    "    \n",
    "#     epoch_train_loss_brian = train_epoch(model_brian, optimizer_brian, criterion_brian, dataloader, m)\n",
    "#     full_data_brian.append([t, epoch_train_loss_brian])\n",
    "    \n",
    "#     if t % checkpoint_rate == 0:\n",
    "#         acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber = test_model(model_amber, dataloader, m, criterion_amber)\n",
    "#         checkpoint_name_amber = f'ethan_m{m}_n{n}_checkpoint{t}.pt'\n",
    "#         checkpoint_data_amber.append([t, checkpoint_name_amber, acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber])\n",
    "#         print(f'Ethan: Epoch: {t}/{epochs+era2epochs}, Test Loss: {epoch_test_loss_amber}, Incorrect: {num_wrong_amber}/1000 examples')\n",
    "#         torch.save(model_amber, f'./models/{checkpoint_name_amber}')\n",
    "        \n",
    "#         acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian = test_model(model_brian, dataloader, m, criterion_brian)\n",
    "#         checkpoint_name_brian = f'fred_m{m}_n{n}_checkpoint{t}.pt'\n",
    "#         checkpoint_data_brian.append([t, checkpoint_name_brian, acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian])\n",
    "#         print(f'Fred: Epoch: {t}/{epochs+era2epochs}, Test Loss: {epoch_test_loss_brian}, Incorrect: {num_wrong_brian}/1000 examples')\n",
    "#         torch.save(model_amber, f'./models/{checkpoint_name_brian}')\n",
    "        \n",
    "# print(\"END OF ERA 2\")\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a47275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data_amber = pd.DataFrame(full_data_amber, columns =['epoch', 'training_loss'])\n",
    "df_full_data_brian = pd.DataFrame(full_data_brian, columns =['epoch', 'training_loss'])\n",
    "\n",
    "df_checkpoint_data_amber = pd.DataFrame(checkpoint_data_amber, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])\n",
    "df_checkpoint_data_brian = pd.DataFrame(checkpoint_data_brian, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0878f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data_amber.to_csv('./logs/greg_full_data.csv')\n",
    "df_full_data_brian.to_csv('./logs/harold_full_data.csv')\n",
    "\n",
    "df_checkpoint_data_amber.to_csv('./logs/greg_checkpoint_data.csv')\n",
    "df_checkpoint_data_brian.to_csv('./logs/harold_checkpoint_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76941c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
