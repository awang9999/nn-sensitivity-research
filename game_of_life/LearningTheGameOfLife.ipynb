{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a064907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "from GOLDataset import generateDataset\n",
    "from GOLCNN import OPNet, train_epoch, test_model\n",
    "from MinimalSolution import MinNet\n",
    "\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70df9be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f07555425d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed everything for reproducibility\n",
    "# seed = 11 for carl and denise\n",
    "# seed = 12 for ethan and fred\n",
    "seed = 12\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbfc8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0, Test Loss: 3.1909209708414432e-18, Correct: 1000/1000, Incorrect: 0/1000\n"
     ]
    }
   ],
   "source": [
    "# Ensure test_model() works on the minimal solution CNN\n",
    "dataset_size = 1000\n",
    "dataloader = generateDataset(dataSetSize=dataset_size, size=32, n_steps=3)\n",
    "min_model = MinNet(3)\n",
    "min_model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "acc, epoch_test_loss, num_correct, num_wrong = test_model(min_model, dataloader, 1, criterion)\n",
    "print(f'Accuracy: {acc}, Test Loss: {epoch_test_loss}, Correct: {num_correct}/{dataset_size}, Incorrect: {num_wrong}/{dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3241bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "dataset_size = 1000\n",
    "datapoint_size = 32\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size_param = 1\n",
    "epochs = 1500\n",
    "era2epochs = 500\n",
    "checkpoint_rate = 100\n",
    "\n",
    "m = 8 # Overparameterization Factor\n",
    "n = 2  # Steps of GOL simulation\n",
    "\n",
    "model_amber = OPNet(m, n)\n",
    "model_brian = OPNet(m, n)\n",
    "\n",
    "criterion_amber = nn.MSELoss()\n",
    "criterion_brian = nn.MSELoss()\n",
    "optimizer_amber = torch.optim.SGD(model_amber.parameters(), lr=learning_rate)\n",
    "optimizer_brian = torch.optim.SGD(model_brian.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c51e6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models loaded to device\n"
     ]
    }
   ],
   "source": [
    "model_amber.to(device)\n",
    "model_brian.to(device)\n",
    "print('models loaded to device')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d195285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethan: Epoch: 100/1500, Test Loss: 0.18208691477775574, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 100/1500, Test Loss: 0.17981183528900146, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 200/1500, Test Loss: 0.17560186982154846, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 200/1500, Test Loss: 0.17270363867282867, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 300/1500, Test Loss: 0.17354384064674377, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 300/1500, Test Loss: 0.16518963873386383, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 400/1500, Test Loss: 0.1682957410812378, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 400/1500, Test Loss: 0.14946874976158142, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 500/1500, Test Loss: 0.15747635066509247, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 500/1500, Test Loss: 0.1390988975763321, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 600/1500, Test Loss: 0.14488430321216583, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 600/1500, Test Loss: 0.13019080460071564, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 700/1500, Test Loss: 0.13092388212680817, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 700/1500, Test Loss: 0.11572571098804474, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 800/1500, Test Loss: 0.11237150430679321, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 800/1500, Test Loss: 0.0698140487074852, Incorrect: 1000/1000 examples\n",
      "Ethan: Epoch: 900/1500, Test Loss: 0.07400757074356079, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 900/1500, Test Loss: 0.006062298081815243, Incorrect: 568/1000 examples\n",
      "Ethan: Epoch: 1000/1500, Test Loss: 0.050336964428424835, Incorrect: 1000/1000 examples\n",
      "Fred: Epoch: 1000/1500, Test Loss: 0.0009591212146915495, Incorrect: 12/1000 examples\n",
      "Ethan: Epoch: 1100/1500, Test Loss: 0.0019537426996976137, Incorrect: 63/1000 examples\n",
      "Fred: Epoch: 1100/1500, Test Loss: 0.0004239059635438025, Incorrect: 0/1000 examples\n",
      "Ethan: Epoch: 1200/1500, Test Loss: 0.0005488104070536792, Incorrect: 1/1000 examples\n",
      "Fred: Epoch: 1200/1500, Test Loss: 0.0002493642386980355, Incorrect: 1/1000 examples\n",
      "Ethan: Epoch: 1300/1500, Test Loss: 0.0002692560665309429, Incorrect: 0/1000 examples\n",
      "Fred: Epoch: 1300/1500, Test Loss: 0.0001715833495836705, Incorrect: 0/1000 examples\n",
      "Ethan: Epoch: 1400/1500, Test Loss: 0.0001695045648375526, Incorrect: 0/1000 examples\n",
      "Fred: Epoch: 1400/1500, Test Loss: 0.00012888891797047108, Incorrect: 0/1000 examples\n",
      "Ethan: Epoch: 1500/1500, Test Loss: 0.00012014071398880333, Incorrect: 0/1000 examples\n",
      "Fred: Epoch: 1500/1500, Test Loss: 0.00010165599087486044, Incorrect: 0/1000 examples\n",
      "END OF ERA 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m optimizer_brian \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model_brian\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39mera2epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mgenerateDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSetSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatapoint_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     epoch_train_loss_amber \u001b[38;5;241m=\u001b[39m train_epoch(model_amber, optimizer_amber, criterion_amber, dataloader, m)\n\u001b[1;32m     41\u001b[0m     full_data_amber\u001b[38;5;241m.\u001b[39mappend([t, epoch_train_loss_amber])\n",
      "File \u001b[0;32m~/Desktop/everything/projects/nn-sensitivity-research/game_of_life/GOLDataset.py:19\u001b[0m, in \u001b[0;36mgenerateDataset\u001b[0;34m(dataSetSize, size, n_steps, returnTensor)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs\u001b[38;5;241m.\u001b[39mappend(start_state)\n\u001b[1;32m     18\u001b[0m engine\u001b[38;5;241m.\u001b[39mgame_state \u001b[38;5;241m=\u001b[39m start_state\n\u001b[0;32m---> 19\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m output_state \u001b[38;5;241m=\u001b[39m engine\u001b[38;5;241m.\u001b[39mgame_state\n\u001b[1;32m     22\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output_state)\n",
      "File \u001b[0;32m~/Desktop/everything/projects/nn-sensitivity-research/game_of_life/GameOfLife.py:29\u001b[0m, in \u001b[0;36mGameOfLifeEngine.step_n\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_n\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m---> 29\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/everything/projects/nn-sensitivity-research/game_of_life/GameOfLife.py:57\u001b[0m, in \u001b[0;36mStandardEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m live_neighbors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_state[i,j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     56\u001b[0m live_neighbors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_state[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, j\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 57\u001b[0m live_neighbors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m live_neighbors \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_state[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m, j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m n \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m live_neighbors \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;66;03m# dies by underpopulation (rule 1)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "full_data_amber = []\n",
    "full_data_brian = []\n",
    "checkpoint_data_amber = []\n",
    "checkpoint_data_brian = []\n",
    "\n",
    "for t in range(1, epochs + 1):\n",
    "    dataloader = generateDataset(dataSetSize=dataset_size, \n",
    "                                 size=datapoint_size, \n",
    "                                 n_steps=n)\n",
    "    \n",
    "    epoch_train_loss_amber = train_epoch(model_amber, optimizer_amber, criterion_amber, dataloader, m)\n",
    "    full_data_amber.append([t, epoch_train_loss_amber])\n",
    "    \n",
    "    epoch_train_loss_brian = train_epoch(model_brian, optimizer_brian, criterion_brian, dataloader, m)\n",
    "    full_data_brian.append([t, epoch_train_loss_brian])\n",
    "    \n",
    "    if t % checkpoint_rate == 0:\n",
    "        acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber = test_model(model_amber, dataloader, m, criterion_amber)\n",
    "        checkpoint_name_amber = f'ethan_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_amber.append([t, checkpoint_name_amber, acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber])\n",
    "        print(f'Ethan: Epoch: {t}/{epochs}, Test Loss: {epoch_test_loss_amber}, Incorrect: {num_wrong_amber}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_amber}')\n",
    "        \n",
    "        acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian = test_model(model_brian, dataloader, m, criterion_brian)\n",
    "        checkpoint_name_brian = f'fred_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_brian.append([t, checkpoint_name_brian, acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian])\n",
    "        print(f'Fred: Epoch: {t}/{epochs}, Test Loss: {epoch_test_loss_brian}, Incorrect: {num_wrong_brian}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_brian}')\n",
    "        \n",
    "print(\"END OF ERA 1\")\n",
    "\n",
    "optimizer_amber = torch.optim.SGD(model_amber.parameters(), lr=learning_rate*0.1)\n",
    "optimizer_brian = torch.optim.SGD(model_brian.parameters(), lr=learning_rate*0.1)\n",
    "\n",
    "for t in range(epochs + 1, epochs+era2epochs+1):\n",
    "    dataloader = generateDataset(dataSetSize=dataset_size, \n",
    "                                 size=datapoint_size, \n",
    "                                 n_steps=n)\n",
    "    \n",
    "    epoch_train_loss_amber = train_epoch(model_amber, optimizer_amber, criterion_amber, dataloader, m)\n",
    "    full_data_amber.append([t, epoch_train_loss_amber])\n",
    "    \n",
    "    epoch_train_loss_brian = train_epoch(model_brian, optimizer_brian, criterion_brian, dataloader, m)\n",
    "    full_data_brian.append([t, epoch_train_loss_brian])\n",
    "    \n",
    "    if t % checkpoint_rate == 0:\n",
    "        acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber = test_model(model_amber, dataloader, m, criterion_amber)\n",
    "        checkpoint_name_amber = f'ethan_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_amber.append([t, checkpoint_name_amber, acc_amber, epoch_test_loss_amber, num_correct_amber, num_wrong_amber])\n",
    "        print(f'Ethan: Epoch: {t}/{epochs+era2epochs}, Test Loss: {epoch_test_loss_amber}, Incorrect: {num_wrong_amber}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_amber}')\n",
    "        \n",
    "        acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian = test_model(model_brian, dataloader, m, criterion_brian)\n",
    "        checkpoint_name_brian = f'fred_m{m}_n{n}_checkpoint{t}.pt'\n",
    "        checkpoint_data_brian.append([t, checkpoint_name_brian, acc_brian, epoch_test_loss_brian, num_correct_brian, num_wrong_brian])\n",
    "        print(f'Fred: Epoch: {t}/{epochs+era2epochs}, Test Loss: {epoch_test_loss_brian}, Incorrect: {num_wrong_brian}/1000 examples')\n",
    "        torch.save(model_amber, f'./models/{checkpoint_name_brian}')\n",
    "        \n",
    "print(\"END OF ERA 2\")\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6a47275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data_amber = pd.DataFrame(full_data_amber, columns =['epoch', 'training_loss'])\n",
    "df_full_data_brian = pd.DataFrame(full_data_brian, columns =['epoch', 'training_loss'])\n",
    "\n",
    "df_checkpoint_data_amber = pd.DataFrame(checkpoint_data_amber, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])\n",
    "df_checkpoint_data_brian = pd.DataFrame(checkpoint_data_brian, columns =['epoch', 'checkpoint_name', 'accuracy', 'test_loss', 'num_correct', 'num_wrong'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0878f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_data_amber.to_csv('./logs/ethan_full_data.csv')\n",
    "df_full_data_brian.to_csv('./logs/fred_full_data.csv')\n",
    "\n",
    "df_checkpoint_data_amber.to_csv('./logs/ethan_checkpoint_data.csv')\n",
    "df_checkpoint_data_brian.to_csv('./logs/fred_checkpoint_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76941c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
